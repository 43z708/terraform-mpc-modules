# Root Terragrunt Configuration
# Shared configuration for all environments with dynamic AWS profile support

locals {
  # Load common configuration
  common_vars = read_terragrunt_config("${get_repo_root()}/examples/terragrunt-infra/common.hcl")
  
  # Load environment-specific version file if it exists, otherwise use root
  current_env_dir = dirname(path_relative_to_include())
  env_version_file = "${get_repo_root()}/examples/terragrunt-infra/${local.current_env_dir}/version.hcl"
  root_version_file = "${get_repo_root()}/examples/terragrunt-infra/version.hcl"
  version_vars = try(
    read_terragrunt_config(local.env_version_file),
    read_terragrunt_config(local.root_version_file)
  )
}

# Remote state configuration with dynamic profile and region
remote_state {
  backend = "s3"
  config = {
    bucket         = local.common_vars.inputs.terraform_state_bucket
    key            = "${path_relative_to_include()}/terraform.tfstate"
    region         = local.common_vars.inputs.aws_region
    encrypt        = true
    profile        = local.common_vars.inputs.aws_profile
    
    # Optional: Add DynamoDB table for state locking
    # dynamodb_table = "terraform-state-lock"
  }
}

# Set environment variables for automatic AWS profile handling
terraform {
  before_hook "set_aws_profile" {
    commands = ["apply", "plan", "destroy", "refresh", "validate", "init", "output"]
    execute  = ["bash", "-c", "export AWS_PROFILE=${local.common_vars.inputs.aws_profile}"]
  }
  
  extra_arguments "common_vars" {
    commands = get_terraform_commands_that_need_vars()
    env_vars = {
      AWS_PROFILE = local.common_vars.inputs.aws_profile
    }
  }
}

# Generate dynamic provider configuration
generate "providers" {
  path      = "providers.tf"
  if_exists = "overwrite"
  contents = <<EOF
# Auto-generated provider configurations
# This file is generated by Terragrunt - DO NOT EDIT MANUALLY
# Environment: ${local.common_vars.inputs.environment}
# AWS Profile: ${local.common_vars.inputs.aws_profile}

terraform {
  required_version = "${local.version_vars.inputs.provider_versions.terraform_version}"
  
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "${local.version_vars.inputs.provider_versions.aws_version}"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "${local.version_vars.inputs.provider_versions.kubernetes_version}"
    }
    random = {
      source  = "hashicorp/random"
      version = "${local.version_vars.inputs.provider_versions.random_version}"
    }
    kubectl = {
      source  = "gavinbunney/kubectl"
      version = "${local.version_vars.inputs.provider_versions.kubectl_version}"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "${local.version_vars.inputs.provider_versions.helm_version}"
    }
  }
  backend "s3" {}
}

provider "aws" {
  region              = "${local.common_vars.inputs.aws_region}"
  profile             = "${local.common_vars.inputs.aws_profile}"
  allowed_account_ids = ["${local.common_vars.inputs.aws_account_id}"]
  
  default_tags {
    tags = {
      Environment = "${local.common_vars.inputs.environment}"
      ManagedBy   = "terragrunt"
      Project     = "${local.common_vars.inputs.project_name}"
    }
  }
}

provider "kubernetes" {
  config_path    = var.kubeconfig_path
  config_context = var.kubeconfig_context
  
  dynamic "exec" {
    for_each = var.use_eks_cluster_authentication ? [1] : []
    content {
      api_version = "client.authentication.k8s.io/v1beta1"
      args        = ["eks", "get-token", "--cluster-name", var.cluster_name, "--region", "${local.common_vars.inputs.aws_region}", "--profile", "${local.common_vars.inputs.aws_profile}"]
      command     = "aws"
    }
  }
}

EOF
} 